from pyspark.sql import SparkSession
from pyspark.sql.functions import col, length, lpad

def init_spark():
    return SparkSession.builder \
        .appName("An치lisis Estructura RENIEC") \
        .config("spark.driver.memory", "8G") \
        .getOrCreate()

def analyze_structure():
    spark = init_spark()
    
    try:
        # Cargar datos
        df = spark.read.csv(
            "RENIEC10GB_CSV.csv",
            sep=";",
            header=True,
            inferSchema=True
        )
        
        # 1. Contar total de filas
        total_filas = df.count()
        print(f"\n游늵 Total de filas en el dataset: {total_filas:,}")
        
        # 2. Analizar campo 'documento'
        # - Asegurar que todos tengan 8 d칤gitos (rellenar con ceros a la izquierda)
        df = df.withColumn("documento_completo", 
                         lpad(col("documento").cast("string"), 8, "0"))
        
        # - Encontrar los 3 documentos m치s peque침os
        print("\n游댷 3 Documentos M츼S PEQUE칌OS:")
        min_docs = df.orderBy("documento_completo").limit(3)
        min_docs.select("documento_completo").show(3, truncate=False)
        
        # - Encontrar los 3 documentos m치s grandes
        print("\n游댶 3 Documentos M츼S GRANDES:")
        max_docs = df.orderBy(col("documento_completo").desc()).limit(3)
        max_docs.select("documento_completo").show(3, truncate=False)
        
        # - Estad칤sticas adicionales
        print("\n游댌 Resumen estad칤stico del campo 'documento':")
        df.select(length("documento_completo").alias("longitud")).groupBy("longitud").count().show()
        
    finally:
        spark.stop()

if __name__ == "__main__":
    analyze_structure()